# ğŸ”¬ Advanced Research Paper Analyzer

An enterprise-grade, privacy-focused Retrieval-Augmented Generation (RAG) application designed for researchers and data scientists. This tool enables local, cost-free analysis of dense academic papers using a hybrid search architecture.



## ğŸŒŸ Key Features

* **Hybrid Search Engine:** Combines **Semantic (Dense)** retrieval via HuggingFace BGE embeddings and **Keyword (Sparse)** retrieval via BM25 to ensure technical terminology is never missed.
* **State-Aware Conversation:** Built with **LangChain Expression Language (LCEL)**, featuring a history-aware retriever that re-contextualizes follow-up questions for precise accuracy.
* **100% Local & Private:** Powered by **Ollama**, ensuring all data stays on your machine with zero API costs or data leakage.
* **Automated Context Management:** Features smart session handling that clears vector memory and chat history upon new document uploads.
* **Citation Transparency:** Provides source-backed responses with exact page-level citations and metadata extraction.

## ğŸ—ï¸ System Architecture

The project follows a modular design pattern to separate concerns between ingestion, retrieval logic, and the user interface.



* **Ingestion Layer:** Utilizes `PyPDF` and `RecursiveCharacterTextSplitter` to maintain semantic coherence across chunks.
* **Vector Store:** Powered by **ChromaDB** for persistent, low-latency embedding storage.
* **LLM Orchestration:** Uses `llama3.1` (via Ollama) and `BAAI/bge-small-en-v1.5` embeddings for high-performance inference on local hardware.

## ğŸ“ Project Structure

```text
research-paper-analyzer/
â”œâ”€â”€ app.py                 # Streamlit UI & State Management
â”œâ”€â”€ requirements.txt       # Version-pinned dependencies
â”œâ”€â”€ engine/                # Core Logic Package
â”‚   â”œâ”€â”€ __init__.py        
â”‚   â”œâ”€â”€ ingestion.py       # PDF Parsing & Chunking
â”‚   â”œâ”€â”€ hybrid_engine.py   # Hybrid Retrieval & LCEL Chains
â”‚   â””â”€â”€ prompts.py         # Versioned ChatPromptTemplates
â””â”€â”€ data/                  # Persistent Vector Storage

```

## ğŸš€ Getting Started

### Prerequisites

* Python 3.10+
* [Ollama](https://ollama.com/) installed and running

### Installation

1. **Pull the LLM:**
```bash
ollama pull llama3.1

```


2. **Clone the Repository:**
```bash
git clone https://github.com/althaffazil/research-paper-analyzer.git
cd research-paper-analyzer

```


3. **Install Dependencies:**
```bash
pip install -r requirements.txt

```


4. **Run the Application:**
```bash
streamlit run app.py

```



## ğŸ› ï¸ Tech Stack

* **UI:** Streamlit
* **Orchestration:** LangChain (LCEL)
* **LLM:** Ollama (Llama 3.1)
* **Embeddings:** HuggingFace BGE Small
* **Vector DB:** ChromaDB
* **Search Algorithms:** BM25 (Rank-BM25), Cosine Similarity
